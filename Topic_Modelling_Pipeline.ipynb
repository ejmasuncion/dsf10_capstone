{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad61a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "from pprint import pprint# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import gensim.models as genmodels\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332bce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b9410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('medium_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d518be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_url</th>\n",
       "      <th>best_of</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>image_count</th>\n",
       "      <th>link_count</th>\n",
       "      <th>blockquote_count</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>reading_time</th>\n",
       "      <th>clap_count</th>\n",
       "      <th>unique_clap_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>author_url</th>\n",
       "      <th>author_name</th>\n",
       "      <th>publication_url</th>\n",
       "      <th>publication_name</th>\n",
       "      <th>codeblock_count</th>\n",
       "      <th>code_count</th>\n",
       "      <th>all_codes_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://medium.com/towards-data-science/unders...</td>\n",
       "      <td>All time</td>\n",
       "      <td>Understanding AUC - ROC Curve</td>\n",
       "      <td>[Image 1] (Image courtesy: My Photoshopped C...</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Jun 27, 2018</td>\n",
       "      <td>5</td>\n",
       "      <td>15949</td>\n",
       "      <td>4149</td>\n",
       "      <td>76</td>\n",
       "      <td>https://medium.com/@narkhedesarang?source=---t...</td>\n",
       "      <td>Sarang Narkhede</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['[]']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://medium.com/towards-data-science/buildi...</td>\n",
       "      <td>All time</td>\n",
       "      <td>Building A Logistic Regression in Python, Step...</td>\n",
       "      <td>Photo Credit: Scikit-Learn Logistic Regressio...</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Sep 29, 2017</td>\n",
       "      <td>9</td>\n",
       "      <td>11829</td>\n",
       "      <td>2671</td>\n",
       "      <td>133</td>\n",
       "      <td>https://actsusanli.medium.com/?source=---three...</td>\n",
       "      <td>Susan Li</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>22</td>\n",
       "      <td>128</td>\n",
       "      <td>['import pandas as pd', 'import numpy as np', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://medium.com/towards-data-science/pca-us...</td>\n",
       "      <td>All time</td>\n",
       "      <td>PCA using Python (scikit-learn)</td>\n",
       "      <td>Original image (left) with Different Amounts ...</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>Dec 5, 2017</td>\n",
       "      <td>8</td>\n",
       "      <td>7248</td>\n",
       "      <td>1624</td>\n",
       "      <td>36</td>\n",
       "      <td>https://medium.com/@GalarnykMichael?source=---...</td>\n",
       "      <td>Michael Galarnyk</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>18</td>\n",
       "      <td>47</td>\n",
       "      <td>['import pandas as pd', 'url = \"https://archiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://medium.com/towards-data-science/train-...</td>\n",
       "      <td>All time</td>\n",
       "      <td>Train/Test Split and Cross Validation in Python</td>\n",
       "      <td>Hi everyone! After my last post on linear reg...</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>May 17, 2017</td>\n",
       "      <td>9</td>\n",
       "      <td>6597</td>\n",
       "      <td>1718</td>\n",
       "      <td>49</td>\n",
       "      <td>https://medium.com/@adi.bronshtein?source=---t...</td>\n",
       "      <td>Adi Bronshtein</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>['import pandas as pd', 'from sklearn import d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://medium.com/towards-data-science/the-5-...</td>\n",
       "      <td>All time</td>\n",
       "      <td>The 5 Clustering Algorithms Data Scientists Ne...</td>\n",
       "      <td>Want to be inspired? Come join my Super Quote...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Feb 6, 2018</td>\n",
       "      <td>11</td>\n",
       "      <td>21321</td>\n",
       "      <td>4502</td>\n",
       "      <td>54</td>\n",
       "      <td>https://medium.com/@george.seif94?source=---th...</td>\n",
       "      <td>George Seif</td>\n",
       "      <td>https://towardsdatascience.com/?source=post_pa...</td>\n",
       "      <td>Towards Data Science</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['[]']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_url   best_of  \\\n",
       "0  https://medium.com/towards-data-science/unders...  All time   \n",
       "1  https://medium.com/towards-data-science/buildi...  All time   \n",
       "2  https://medium.com/towards-data-science/pca-us...  All time   \n",
       "3  https://medium.com/towards-data-science/train-...  All time   \n",
       "4  https://medium.com/towards-data-science/the-5-...  All time   \n",
       "\n",
       "                                               title  \\\n",
       "0                      Understanding AUC - ROC Curve   \n",
       "1  Building A Logistic Regression in Python, Step...   \n",
       "2                    PCA using Python (scikit-learn)   \n",
       "3    Train/Test Split and Cross Validation in Python   \n",
       "4  The 5 Clustering Algorithms Data Scientists Ne...   \n",
       "\n",
       "                                             summary  image_count  link_count  \\\n",
       "0    [Image 1] (Image courtesy: My Photoshopped C...           13           4   \n",
       "1   Photo Credit: Scikit-Learn Logistic Regressio...           23          10   \n",
       "2   Original image (left) with Different Amounts ...            8          14   \n",
       "3   Hi everyone! After my last post on linear reg...            6          14   \n",
       "4   Want to be inspired? Come join my Super Quote...            8           1   \n",
       "\n",
       "   blockquote_count publication_date  reading_time  clap_count  \\\n",
       "0                 2     Jun 27, 2018             5       15949   \n",
       "1                 0     Sep 29, 2017             9       11829   \n",
       "2                 0      Dec 5, 2017             8        7248   \n",
       "3                 0     May 17, 2017             9        6597   \n",
       "4                 1      Feb 6, 2018            11       21321   \n",
       "\n",
       "   unique_clap_count  comment_count  \\\n",
       "0               4149             76   \n",
       "1               2671            133   \n",
       "2               1624             36   \n",
       "3               1718             49   \n",
       "4               4502             54   \n",
       "\n",
       "                                          author_url       author_name  \\\n",
       "0  https://medium.com/@narkhedesarang?source=---t...   Sarang Narkhede   \n",
       "1  https://actsusanli.medium.com/?source=---three...          Susan Li   \n",
       "2  https://medium.com/@GalarnykMichael?source=---...  Michael Galarnyk   \n",
       "3  https://medium.com/@adi.bronshtein?source=---t...    Adi Bronshtein   \n",
       "4  https://medium.com/@george.seif94?source=---th...       George Seif   \n",
       "\n",
       "                                     publication_url      publication_name  \\\n",
       "0  https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "1  https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "2  https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "3  https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "4  https://towardsdatascience.com/?source=post_pa...  Towards Data Science   \n",
       "\n",
       "   codeblock_count  code_count  \\\n",
       "0                0           0   \n",
       "1               22         128   \n",
       "2               18          47   \n",
       "3                0          47   \n",
       "4                0           0   \n",
       "\n",
       "                                      all_codes_list  \n",
       "0                                             ['[]']  \n",
       "1  ['import pandas as pd', 'import numpy as np', ...  \n",
       "2  ['import pandas as pd', 'url = \"https://archiv...  \n",
       "3  ['import pandas as pd', 'from sklearn import d...  \n",
       "4                                             ['[]']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7579150b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:7: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:5: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:7: DeprecationWarning: invalid escape sequence \\s\n",
      "C:\\Users\\colee\\AppData\\Local\\Temp\\ipykernel_15936\\2013857722.py:5: DeprecationWarning: invalid escape sequence \\S\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
      "C:\\Users\\colee\\AppData\\Local\\Temp\\ipykernel_15936\\2013857722.py:7: DeprecationWarning: invalid escape sequence \\s\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' [Image 1] (My Photoshopped Collection) In Machine Learning, performance measurement is an essential task. So when it comes to a classification problem, we can count on an AUC - ROC Curve. When we need to check or visualize the performance of the multi-class classification problem, we use the AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve. It is one of the most important evaluation metrics for checking any classification model’s performance. It is also written as AUROC (Area Under the Receiver Operating Characteristics) Note: For better understanding, I suggest you read my article about Confusion Matrix. This blog aims to answer the following questions: 1. What is the AUC - ROC Curve? 2. Defining terms used in AUC and ROC Curve. 3. How to speculate the performance of the model? 4. Relation between Sensitivity, Specificity, FPR, and Threshold. 5. How to use AUC - ROC curve for the multiclass model? What is the AUC - ROC Curve? AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease. The ROC curve is plotted with TPR against the FPR where TPR is on the y-axis and FPR is on the x-axis. AUC - ROC Curve [Image 2] (My Photoshopped Collection) Defining terms used in AUC and ROC Curve. TPR (True Positive Rate) / Recall /Sensitivity Image 3 Specificity Image 4 FPR Image 5 How to speculate about the performance of the model? An excellent model has AUC near to the 1 which means it has a good measure of separability. A poor model has an AUC near 0 which means it has the worst measure of separability. In fact, it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5, it means the model has no class separation capacity whatsoever. Let’s interpret the above statements. As we know, ROC is a curve of probability. So lets plot the distributions of those probabilities: Note: Red distribution curve is of the positive class (patients with disease) and the green distribution curve is of the negative class(patients with no disease). [Image 6 and 7] (My Photoshopped Collection) This is an ideal situation. When two curves don’t overlap at all means model has an ideal measure of separability. It is perfectly able to distinguish between positive class and negative class. [Image 8 and 9] (My Photoshopped Collection) When two distributions overlap, we introduce type 1 and type 2 errors. Depending upon the threshold, we can minimize or maximize them. When AUC is 0.7, it means there is a 70% chance that the model will be able to distinguish between positive class and negative class. [Image 10 and 11] (My Photoshopped Collection) This is the worst situation. When AUC is approximately 0.5, the model has no discrimination capacity to distinguish between positive class and negative class. [Image 12 and 13] (My Photoshopped Collection) When AUC is approximately 0, the model is actually reciprocating the classes. It means the model is predicting a negative class as a positive class and vice versa. The relation between Sensitivity, Specificity, FPR, and Threshold. Sensitivity and Specificity are inversely proportional to each other. So when we increase Sensitivity, Specificity decreases, and vice versa. Sensitivity️, Specificity️ and Sensitivity️, Specificity️ When we decrease the threshold, we get more positive values thus it increases the sensitivity and decreasing the specificity. Similarly, when we increase the threshold, we get more negative values thus we get higher specificity and lower sensitivity. As we know FPR is 1 - specificity. So when we increase TPR, FPR also increases and vice versa. TPR️, FPR️ and TPR️, FPR️ How to use the AUC ROC curve for the multi-class model? In a multi-class model, we can plot the N number of AUC ROC Curves for N number classes using the One vs ALL methodology. So for example, If you have three classes named X, Y, and Z, you will have one ROC for X classified against Y and Z, another ROC for Y classified against X and Z, and the third one of Z classified against Y and X. Thanks for Reading. I hope I’ve given you some understanding of what exactly is the AUC - ROC Curve. If you like this post, a tad of extra motivation will be helpful by giving this post some claps . I am always open to your questions and suggestions. You can share this on Facebook, Twitter, Linkedin, so someone in need might stumble upon this. You can reach me at: LinkedIn: Twitter: Github: ']\n"
     ]
    }
   ],
   "source": [
    "data = df.summary.values.tolist()\n",
    "# Convert to list \n",
    "data = df.summary.values.tolist()  \n",
    "# Remove Emails \n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]  \n",
    "# Remove new line characters \n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]  \n",
    "# Remove distracting single quotes \n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "#Remove \\n\n",
    "data = [re.sub(\"\\n\", \"\", sent) for sent in data]\n",
    "data = [re.sub(\"(Image courtesy: )\", \"\", sent) for sent in data]\n",
    "#Remove links\n",
    "data = [re.sub(r'http\\S+', '', sent) for sent in data]\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a876f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing the pipeline (to remove once data is here)\n",
    "# df=pd.read_json('newsgroups.json')\n",
    "# print(df.target_names.unique())\n",
    "# df.head()\n",
    "# # Convert to list \n",
    "# data = df.content.values.tolist()\n",
    "\n",
    "# # Remove Emails \n",
    "# data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]  \n",
    "# data = [re.sub('\\s+', ' ', sent) for sent in data]  \n",
    "# # Remove distracting single quotes \n",
    "# data = [re.sub(\"\\'\", \"\", sent) for sent in data]  \n",
    "# pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03ca03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loeading Cleaned Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ebf56",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb124c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['image', 'my', 'photoshopped', 'collection', 'in', 'machine', 'learning', 'performance', 'measurement', 'is', 'an', 'essential', 'task', 'so', 'when', 'it', 'comes', 'to', 'classification', 'problem', 'we', 'can', 'count', 'on', 'an', 'auc', 'roc', 'curve', 'when', 'we', 'need', 'to', 'check', 'or', 'visualize', 'the', 'performance', 'of', 'the', 'multi', 'class', 'classification', 'problem', 'we', 'use', 'the', 'auc', 'area', 'under', 'the', 'curve', 'roc', 'receiver', 'operating', 'characteristics', 'curve', 'it', 'is', 'one', 'of', 'the', 'most', 'important', 'evaluation', 'metrics', 'for', 'checking', 'any', 'classification', 'model', 'performance', 'it', 'is', 'also', 'written', 'as', 'auroc', 'area', 'under', 'the', 'receiver', 'operating', 'characteristics', 'note', 'for', 'better', 'understanding', 'suggest', 'you', 'read', 'my', 'article', 'about', 'confusion', 'matrix', 'this', 'blog', 'aims', 'to', 'answer', 'the', 'following', 'questions', 'what', 'is', 'the', 'auc', 'roc', 'curve', 'defining', 'terms', 'used', 'in', 'auc', 'and', 'roc', 'curve', 'how', 'to', 'speculate', 'the', 'performance', 'of', 'the', 'model', 'relation', 'between', 'sensitivity', 'specificity', 'fpr', 'and', 'threshold', 'how', 'to', 'use', 'auc', 'roc', 'curve', 'for', 'the', 'multiclass', 'model', 'what', 'is', 'the', 'auc', 'roc', 'curve', 'auc', 'roc', 'curve', 'is', 'performance', 'measurement', 'for', 'the', 'classification', 'problems', 'at', 'various', 'threshold', 'settings', 'roc', 'is', 'probability', 'curve', 'and', 'auc', 'represents', 'the', 'degree', 'or', 'measure', 'of', 'separability', 'it', 'tells', 'how', 'much', 'the', 'model', 'is', 'capable', 'of', 'distinguishing', 'between', 'classes', 'higher', 'the', 'auc', 'the', 'better', 'the', 'model', 'is', 'at', 'predicting', 'classes', 'as', 'and', 'classes', 'as', 'by', 'analogy', 'the', 'higher', 'the', 'auc', 'the', 'better', 'the', 'model', 'is', 'at', 'distinguishing', 'between', 'patients', 'with', 'the', 'disease', 'and', 'no', 'disease', 'the', 'roc', 'curve', 'is', 'plotted', 'with', 'tpr', 'against', 'the', 'fpr', 'where', 'tpr', 'is', 'on', 'the', 'axis', 'and', 'fpr', 'is', 'on', 'the', 'axis', 'auc', 'roc', 'curve', 'image', 'my', 'photoshopped', 'collection', 'defining', 'terms', 'used', 'in', 'auc', 'and', 'roc', 'curve', 'tpr', 'true', 'positive', 'rate', 'recall', 'sensitivity', 'image', 'specificity', 'image', 'fpr', 'image', 'how', 'to', 'speculate', 'about', 'the', 'performance', 'of', 'the', 'model', 'an', 'excellent', 'model', 'has', 'auc', 'near', 'to', 'the', 'which', 'means', 'it', 'has', 'good', 'measure', 'of', 'separability', 'poor', 'model', 'has', 'an', 'auc', 'near', 'which', 'means', 'it', 'has', 'the', 'worst', 'measure', 'of', 'separability', 'in', 'fact', 'it', 'means', 'it', 'is', 'reciprocating', 'the', 'result', 'it', 'is', 'predicting', 'as', 'and', 'as', 'and', 'when', 'auc', 'is', 'it', 'means', 'the', 'model', 'has', 'no', 'class', 'separation', 'capacity', 'whatsoever', 'let', 'interpret', 'the', 'above', 'statements', 'as', 'we', 'know', 'roc', 'is', 'curve', 'of', 'probability', 'so', 'lets', 'plot', 'the', 'distributions', 'of', 'those', 'probabilities', 'note', 'red', 'distribution', 'curve', 'is', 'of', 'the', 'positive', 'class', 'patients', 'with', 'disease', 'and', 'the', 'green', 'distribution', 'curve', 'is', 'of', 'the', 'negative', 'class', 'patients', 'with', 'no', 'disease', 'image', 'and', 'my', 'photoshopped', 'collection', 'this', 'is', 'an', 'ideal', 'situation', 'when', 'two', 'curves', 'don', 'overlap', 'at', 'all', 'means', 'model', 'has', 'an', 'ideal', 'measure', 'of', 'separability', 'it', 'is', 'perfectly', 'able', 'to', 'distinguish', 'between', 'positive', 'class', 'and', 'negative', 'class', 'image', 'and', 'my', 'photoshopped', 'collection', 'when', 'two', 'distributions', 'overlap', 'we', 'introduce', 'type', 'and', 'type', 'errors', 'depending', 'upon', 'the', 'threshold', 'we', 'can', 'minimize', 'or', 'maximize', 'them', 'when', 'auc', 'is', 'it', 'means', 'there', 'is', 'chance', 'that', 'the', 'model', 'will', 'be', 'able', 'to', 'distinguish', 'between', 'positive', 'class', 'and', 'negative', 'class', 'image', 'and', 'my', 'photoshopped', 'collection', 'this', 'is', 'the', 'worst', 'situation', 'when', 'auc', 'is', 'approximately', 'the', 'model', 'has', 'no', 'discrimination', 'capacity', 'to', 'distinguish', 'between', 'positive', 'class', 'and', 'negative', 'class', 'image', 'and', 'my', 'photoshopped', 'collection', 'when', 'auc', 'is', 'approximately', 'the', 'model', 'is', 'actually', 'reciprocating', 'the', 'classes', 'it', 'means', 'the', 'model', 'is', 'predicting', 'negative', 'class', 'as', 'positive', 'class', 'and', 'vice', 'versa', 'the', 'relation', 'between', 'sensitivity', 'specificity', 'fpr', 'and', 'threshold', 'sensitivity', 'and', 'specificity', 'are', 'inversely', 'proportional', 'to', 'each', 'other', 'so', 'when', 'we', 'increase', 'sensitivity', 'specificity', 'decreases', 'and', 'vice', 'versa', 'sensitivity', 'specificity', 'and', 'sensitivity', 'specificity', 'when', 'we', 'decrease', 'the', 'threshold', 'we', 'get', 'more', 'positive', 'values', 'thus', 'it', 'increases', 'the', 'sensitivity', 'and', 'decreasing', 'the', 'specificity', 'similarly', 'when', 'we', 'increase', 'the', 'threshold', 'we', 'get', 'more', 'negative', 'values', 'thus', 'we', 'get', 'higher', 'specificity', 'and', 'lower', 'sensitivity', 'as', 'we', 'know', 'fpr', 'is', 'specificity', 'so', 'when', 'we', 'increase', 'tpr', 'fpr', 'also', 'increases', 'and', 'vice', 'versa', 'tpr', 'fpr', 'and', 'tpr', 'fpr', 'how', 'to', 'use', 'the', 'auc', 'roc', 'curve', 'for', 'the', 'multi', 'class', 'model', 'in', 'multi', 'class', 'model', 'we', 'can', 'plot', 'the', 'number', 'of', 'auc', 'roc', 'curves', 'for', 'number', 'classes', 'using', 'the', 'one', 'vs', 'all', 'methodology', 'so', 'for', 'example', 'if', 'you', 'have', 'three', 'classes', 'named', 'and', 'you', 'will', 'have', 'one', 'roc', 'for', 'classified', 'against', 'and', 'another', 'roc', 'for', 'classified', 'against', 'and', 'and', 'the', 'third', 'one', 'of', 'classified', 'against', 'and', 'thanks', 'for', 'reading', 'hope', 've', 'given', 'you', 'some', 'understanding', 'of', 'what', 'exactly', 'is', 'the', 'auc', 'roc', 'curve', 'if', 'you', 'like', 'this', 'post', 'tad', 'of', 'extra', 'motivation', 'will', 'be', 'helpful', 'by', 'giving', 'this', 'post', 'some', 'claps', 'am', 'always', 'open', 'to', 'your', 'questions', 'and', 'suggestions', 'you', 'can', 'share', 'this', 'on', 'facebook', 'twitter', 'linkedin', 'so', 'someone', 'in', 'need', 'might', 'stumble', 'upon', 'this', 'you', 'can', 'reach', 'me', 'at', 'linkedin', 'twitter', 'github']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            #deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3e6d2",
   "metadata": {},
   "source": [
    "### Building the bigram and trigram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c284b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b5a89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'my_photoshopped', 'collection', 'in', 'machine', 'learning', 'performance', 'measurement', 'is', 'an', 'essential', 'task', 'so', 'when', 'it', 'comes', 'to', 'classification', 'problem', 'we', 'can', 'count', 'on', 'an', 'auc_roc_curve', 'when', 'we', 'need', 'to', 'check', 'or', 'visualize', 'the', 'performance', 'of', 'the', 'multi', 'class', 'classification', 'problem', 'we', 'use', 'the', 'auc', 'area_under', 'the', 'curve', 'roc', 'receiver_operating', 'characteristics', 'curve', 'it', 'is', 'one', 'of', 'the', 'most', 'important', 'evaluation', 'metrics', 'for', 'checking', 'any', 'classification', 'model', 'performance', 'it', 'is', 'also', 'written', 'as', 'auroc', 'area_under', 'the', 'receiver_operating', 'characteristics', 'note', 'for', 'better', 'understanding', 'suggest', 'you', 'read', 'my', 'article', 'about', 'confusion_matrix', 'this', 'blog', 'aims', 'to', 'answer', 'the', 'following', 'questions', 'what', 'is', 'the', 'auc_roc_curve', 'defining', 'terms', 'used', 'in', 'auc', 'and', 'roc_curve', 'how', 'to', 'speculate', 'the', 'performance', 'of', 'the', 'model', 'relation', 'between', 'sensitivity', 'specificity', 'fpr', 'and', 'threshold', 'how', 'to', 'use', 'auc_roc_curve', 'for', 'the', 'multiclass', 'model', 'what', 'is', 'the', 'auc_roc_curve', 'auc_roc_curve', 'is', 'performance', 'measurement', 'for', 'the', 'classification', 'problems', 'at', 'various', 'threshold', 'settings', 'roc', 'is', 'probability', 'curve', 'and', 'auc', 'represents', 'the', 'degree', 'or', 'measure', 'of', 'separability', 'it', 'tells', 'how', 'much', 'the', 'model', 'is', 'capable', 'of', 'distinguishing', 'between', 'classes', 'higher', 'the', 'auc', 'the', 'better', 'the', 'model', 'is', 'at', 'predicting', 'classes', 'as', 'and', 'classes', 'as', 'by', 'analogy', 'the', 'higher', 'the', 'auc', 'the', 'better', 'the', 'model', 'is', 'at', 'distinguishing', 'between', 'patients', 'with', 'the', 'disease', 'and', 'no', 'disease', 'the', 'roc_curve', 'is', 'plotted', 'with', 'tpr', 'against', 'the', 'fpr', 'where', 'tpr', 'is', 'on', 'the', 'axis', 'and', 'fpr', 'is', 'on', 'the', 'axis', 'auc_roc_curve', 'image', 'my_photoshopped', 'collection', 'defining', 'terms', 'used', 'in', 'auc', 'and', 'roc_curve', 'tpr', 'true', 'positive', 'rate', 'recall', 'sensitivity', 'image', 'specificity', 'image', 'fpr', 'image', 'how', 'to', 'speculate', 'about', 'the', 'performance', 'of', 'the', 'model', 'an', 'excellent', 'model', 'has', 'auc', 'near', 'to', 'the', 'which', 'means', 'it', 'has', 'good', 'measure', 'of', 'separability', 'poor', 'model', 'has', 'an', 'auc', 'near', 'which', 'means', 'it', 'has', 'the', 'worst', 'measure', 'of', 'separability', 'in', 'fact', 'it', 'means', 'it', 'is', 'reciprocating', 'the', 'result', 'it', 'is', 'predicting', 'as', 'and', 'as', 'and', 'when', 'auc', 'is', 'it', 'means', 'the', 'model', 'has', 'no', 'class', 'separation', 'capacity', 'whatsoever', 'let', 'interpret', 'the', 'above', 'statements', 'as', 'we', 'know', 'roc', 'is', 'curve', 'of', 'probability', 'so', 'lets', 'plot', 'the', 'distributions', 'of', 'those', 'probabilities', 'note', 'red', 'distribution', 'curve', 'is', 'of', 'the', 'positive', 'class', 'patients', 'with', 'disease', 'and', 'the', 'green', 'distribution', 'curve', 'is', 'of', 'the', 'negative', 'class', 'patients', 'with', 'no', 'disease', 'image', 'and', 'my_photoshopped', 'collection', 'this', 'is', 'an', 'ideal', 'situation', 'when', 'two', 'curves', 'don', 'overlap', 'at', 'all', 'means', 'model', 'has', 'an', 'ideal', 'measure', 'of', 'separability', 'it', 'is', 'perfectly', 'able', 'to', 'distinguish', 'between', 'positive', 'class', 'and', 'negative', 'class', 'image', 'and', 'my_photoshopped', 'collection', 'when', 'two', 'distributions', 'overlap', 'we', 'introduce', 'type', 'and', 'type', 'errors', 'depending_upon', 'the', 'threshold', 'we', 'can', 'minimize', 'or', 'maximize', 'them', 'when', 'auc', 'is', 'it', 'means', 'there', 'is', 'chance', 'that', 'the', 'model', 'will', 'be', 'able', 'to', 'distinguish', 'between', 'positive', 'class', 'and', 'negative', 'class', 'image', 'and', 'my_photoshopped', 'collection', 'this', 'is', 'the', 'worst', 'situation', 'when', 'auc', 'is', 'approximately', 'the', 'model', 'has', 'no', 'discrimination', 'capacity', 'to', 'distinguish', 'between', 'positive', 'class', 'and', 'negative', 'class', 'image', 'and', 'my_photoshopped', 'collection', 'when', 'auc', 'is', 'approximately', 'the', 'model', 'is', 'actually', 'reciprocating', 'the', 'classes', 'it', 'means', 'the', 'model', 'is', 'predicting', 'negative', 'class', 'as', 'positive', 'class', 'and', 'vice_versa', 'the', 'relation', 'between', 'sensitivity', 'specificity', 'fpr', 'and', 'threshold', 'sensitivity', 'and', 'specificity', 'are', 'inversely', 'proportional', 'to', 'each', 'other', 'so', 'when', 'we', 'increase', 'sensitivity', 'specificity', 'decreases', 'and', 'vice_versa', 'sensitivity', 'specificity', 'and', 'sensitivity', 'specificity', 'when', 'we', 'decrease', 'the', 'threshold', 'we', 'get', 'more', 'positive', 'values', 'thus', 'it', 'increases', 'the', 'sensitivity', 'and', 'decreasing', 'the', 'specificity', 'similarly', 'when', 'we', 'increase', 'the', 'threshold', 'we', 'get', 'more', 'negative', 'values', 'thus', 'we', 'get', 'higher', 'specificity', 'and', 'lower', 'sensitivity', 'as', 'we', 'know', 'fpr', 'is', 'specificity', 'so', 'when', 'we', 'increase', 'tpr', 'fpr', 'also', 'increases', 'and', 'vice_versa', 'tpr', 'fpr', 'and', 'tpr', 'fpr', 'how', 'to', 'use', 'the', 'auc_roc_curve', 'for', 'the', 'multi', 'class', 'model', 'in', 'multi', 'class', 'model', 'we', 'can', 'plot', 'the', 'number', 'of', 'auc_roc', 'curves', 'for', 'number', 'classes', 'using', 'the', 'one', 'vs', 'all', 'methodology', 'so', 'for', 'example', 'if', 'you', 'have', 'three', 'classes', 'named', 'and', 'you', 'will', 'have', 'one', 'roc', 'for', 'classified', 'against', 'and', 'another', 'roc', 'for', 'classified', 'against', 'and', 'and', 'the', 'third', 'one', 'of', 'classified', 'against', 'and', 'thanks', 'for', 'reading', 'hope', 've', 'given', 'you', 'some', 'understanding', 'of', 'what', 'exactly', 'is', 'the', 'auc_roc_curve', 'if', 'you', 'like', 'this', 'post', 'tad', 'of', 'extra', 'motivation', 'will', 'be', 'helpful', 'by', 'giving', 'this', 'post', 'some', 'claps', 'am', 'always', 'open', 'to', 'your', 'questions', 'and', 'suggestions', 'you', 'can', 'share', 'this', 'on', 'facebook', 'twitter_linkedin', 'so', 'someone', 'in', 'need', 'might', 'stumble', 'upon', 'this', 'you', 'can', 'reach', 'me', 'at', 'linkedin', 'twitter', 'github']\n"
     ]
    }
   ],
   "source": [
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec5f5e",
   "metadata": {},
   "source": [
    "### Define function for stopwords, bigrams, trigrams and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5dd33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fc05e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "data_words_trigrams = make_trigrams(data_words_nostops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95439579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\colee\\anaconda3\\lib\\site-packages\\spacy\\language.py:1895: UserWarning: [W123] Argument disable with value ['parser', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['image', 'photoshopped_collection', 'machine', 'learn', 'performance', 'measurement', 'essential', 'task', 'come', 'classification', 'problem', 'count', 'curve', 'check', 'performance', 'class', 'classification', 'problem', 'auc', 'area', 'curve', 'characteristic', 'curve', 'important', 'evaluation', 'metric', 'check', 'classification', 'model', 'performance', 'also', 'write', 'auroc', 'area', 'receiver_operate', 'characteristic', 'note', 'well', 'understanding', 'suggest', 'read', 'article', 'blog', 'aim', 'answer', 'follow', 'question', 'curve', 'define', 'term', 'use', 'curve', 'speculate', 'performance', 'model', 'relation', 'sensitivity', 'specificity', 'fpr', 'threshold', 'multiclass', 'performance', 'measurement', 'classification', 'problem', 'various', 'threshold', 'setting', 'curve', 'auc', 'represent', 'degree', 'measure', 'separability', 'tell', 'much', 'model', 'capable', 'distinguishing', 'class', 'high', 'auc', 'well', 'model', 'predict', 'class', 'class', 'analogy', 'high', 'auc', 'well', 'model', 'distinguish', 'patient', 'disease', 'disease', 'roc_curve', 'plot', 'tpr', 'fpr', 'axis', 'curve', 'image', 'photoshopped_collection', 'define', 'term', 'use', 'curve', 'tpr', 'true', 'positive', 'rate', 'recall', 'sensitivity', 'image', 'specificity', 'image', 'fpr', 'image', 'speculate', 'performance', 'model', 'excellent', 'model', 'auc', 'mean', 'good', 'measure', 'separability', 'poor', 'model', 'auc', 'mean', 'bad', 'measure', 'separability', 'fact', 'mean', 'reciprocate', 'result', 'predict', 'auc', 'mean', 'model', 'class', 'separation', 'capacity', 'whatsoever', 'let', 'interpret', 'statement', 'know', 'roc_curve', 'probability', 'let', 'plot', 'distribution', 'probability', 'note', 'red', 'distribution', 'curve', 'positive', 'class', 'patient', 'disease', 'green', 'distribution', 'curve', 'negative', 'class', 'patient', 'disease', 'image', 'photoshopped_collection', 'ideal', 'situation', 'curve', 'overlap', 'mean', 'model', 'ideal', 'measure', 'separability', 'perfectly', 'able', 'distinguish', 'positive', 'class', 'negative', 'class', 'image', 'photoshopped_collection', 'distribution', 'overlap', 'introduce', 'type', 'type', 'error', 'threshold', 'minimize', 'auc', 'mean', 'chance', 'model', 'able', 'distinguish', 'positive', 'class', 'negative', 'class', 'image', 'photoshopped_collection', 'bad', 'situation', 'auc', 'approximately', 'model', 'discrimination', 'capacity', 'distinguish', 'positive', 'class', 'negative', 'class', 'image', 'photoshopped_collection', 'auc', 'approximately', 'model', 'actually', 'reciprocate', 'class', 'mean', 'model', 'predict', 'negative', 'class', 'positive', 'class', 'vice_versa', 'relation', 'sensitivity', 'specificity', 'fpr', 'threshold', 'sensitivity', 'specificity', 'inversely', 'proportional', 'increase', 'sensitivity', 'specificity', 'decrease', 'sensitivity', 'specificity', 'sensitivity', 'specificity', 'decrease', 'threshold', 'get', 'positive', 'value', 'thus', 'increase', 'sensitivity', 'decrease', 'specificity', 'similarly', 'increase', 'threshold', 'get', 'negative', 'value', 'thus', 'get', 'high', 'specificity', 'low', 'sensitivity', 'know', 'fpr', 'specificity', 'increase', 'tpr', 'fpr', 'also', 'increase', 'vice_versa', 'tpr', 'fpr', 'class', 'model', 'class', 'model', 'plot', 'number', 'curve', 'number', 'class', 'use', 'methodology', 'example', 'class', 'name', 'classify', 'classify', 'third', 'classified', 'thank', 'read', 'hope', 'give', 'understand', 'exactly', 'curve', 'post', 'tad', 'extra', 'motivation', 'helpful', 'give', 'post', 'clap', 'always', 'open', 'question', 'suggestion', 'share', 'facebook', 'twitter_linkedin', 'need', 'stumble', 'reach', 'linkedin', 'twitter']]\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized_bi = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "data_lemmatized_tri = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized_bi[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4617fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_bi = corpora.Dictionary(data_lemmatized_bi)  \n",
    "id2word_tri = corpora.Dictionary(data_lemmatized_tri)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2bf57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_bi = data_lemmatized_bi\n",
    "texts_tri = data_lemmatized_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41dd7e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Document Frequency \n",
    "corpus_bi = [id2word_bi.doc2bow(text) for text in texts_bi]  \n",
    "corpus_tri = [id2word_tri.doc2bow(text) for text in texts_tri]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d91ab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 1), (7, 2), (8, 2), (9, 1), (10, 10), (11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 2), (17, 1), (18, 2), (19, 2), (20, 1), (21, 20), (22, 4), (23, 1), (24, 2), (25, 1), (26, 1), (27, 13), (28, 3), (29, 2), (30, 1), (31, 1), (32, 4), (33, 4), (34, 1), (35, 4), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 7), (47, 3), (48, 2), (49, 1), (50, 1), (51, 1), (52, 3), (53, 1), (54, 2), (55, 9), (56, 1), (57, 5), (58, 1), (59, 1), (60, 1), (61, 2), (62, 1), (63, 2), (64, 1), (65, 1), (66, 1), (67, 7), (68, 4), (69, 2), (70, 1), (71, 1), (72, 1), (73, 16), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 6), (80, 2), (81, 2), (82, 1), (83, 2), (84, 3), (85, 1), (86, 6), (87, 6), (88, 3), (89, 1), (90, 7), (91, 2), (92, 3), (93, 2), (94, 3), (95, 1), (96, 2), (97, 1), (98, 1), (99, 2), (100, 1), (101, 1), (102, 2), (103, 1), (104, 2), (105, 1), (106, 1), (107, 2), (108, 9), (109, 4), (110, 1), (111, 1), (112, 1), (113, 1), (114, 2), (115, 10), (116, 2), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 2), (125, 1), (126, 1), (127, 6), (128, 2), (129, 4), (130, 1), (131, 1), (132, 1), (133, 2), (134, 1), (135, 1), (136, 3), (137, 2), (138, 1), (139, 2), (140, 3), (141, 1), (142, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# View \n",
    "print(corpus_bi[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ec11866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('able', 2),\n",
       "  ('actually', 1),\n",
       "  ('aim', 1),\n",
       "  ('also', 2),\n",
       "  ('always', 1),\n",
       "  ('analogy', 1),\n",
       "  ('answer', 1),\n",
       "  ('approximately', 2),\n",
       "  ('area', 2),\n",
       "  ('article', 1),\n",
       "  ('auc', 10),\n",
       "  ('auc_roc_curve', 10),\n",
       "  ('auroc', 1),\n",
       "  ('axis', 2),\n",
       "  ('bad', 2),\n",
       "  ('blog', 1),\n",
       "  ('capable', 1),\n",
       "  ('capacity', 2),\n",
       "  ('chance', 1),\n",
       "  ('characteristic', 2),\n",
       "  ('check', 2),\n",
       "  ('clap', 1),\n",
       "  ('class', 20),\n",
       "  ('classification', 4),\n",
       "  ('classified', 1),\n",
       "  ('classify', 2),\n",
       "  ('come', 1),\n",
       "  ('count', 1),\n",
       "  ('curve', 7),\n",
       "  ('decrease', 3),\n",
       "  ('define', 2),\n",
       "  ('degree', 1),\n",
       "  ('discrimination', 1),\n",
       "  ('disease', 4),\n",
       "  ('distinguish', 4),\n",
       "  ('distinguishing', 1),\n",
       "  ('distribution', 4),\n",
       "  ('error', 1),\n",
       "  ('essential', 1),\n",
       "  ('evaluation', 1),\n",
       "  ('exactly', 1),\n",
       "  ('example', 1),\n",
       "  ('excellent', 1),\n",
       "  ('extra', 1),\n",
       "  ('facebook', 1),\n",
       "  ('fact', 1),\n",
       "  ('follow', 1),\n",
       "  ('fpr', 7),\n",
       "  ('get', 3),\n",
       "  ('give', 2),\n",
       "  ('good', 1),\n",
       "  ('green', 1),\n",
       "  ('helpful', 1),\n",
       "  ('high', 3),\n",
       "  ('hope', 1),\n",
       "  ('ideal', 2),\n",
       "  ('image', 9),\n",
       "  ('important', 1),\n",
       "  ('increase', 5),\n",
       "  ('interpret', 1),\n",
       "  ('introduce', 1),\n",
       "  ('inversely', 1),\n",
       "  ('know', 2),\n",
       "  ('learn', 1),\n",
       "  ('let', 2),\n",
       "  ('linkedin', 1),\n",
       "  ('low', 1),\n",
       "  ('machine', 1),\n",
       "  ('mean', 7),\n",
       "  ('measure', 4),\n",
       "  ('measurement', 2),\n",
       "  ('methodology', 1),\n",
       "  ('metric', 1),\n",
       "  ('minimize', 1),\n",
       "  ('model', 16),\n",
       "  ('motivation', 1),\n",
       "  ('much', 1),\n",
       "  ('multiclass', 1),\n",
       "  ('name', 1),\n",
       "  ('need', 2),\n",
       "  ('negative', 6),\n",
       "  ('note', 2),\n",
       "  ('number', 2),\n",
       "  ('open', 1),\n",
       "  ('overlap', 2),\n",
       "  ('patient', 3),\n",
       "  ('perfectly', 1),\n",
       "  ('performance', 6),\n",
       "  ('photoshopped_collection', 6),\n",
       "  ('plot', 3),\n",
       "  ('poor', 1),\n",
       "  ('positive', 7),\n",
       "  ('post', 1),\n",
       "  ('predict', 3),\n",
       "  ('probability', 2),\n",
       "  ('problem', 3),\n",
       "  ('proportional', 1),\n",
       "  ('question', 2),\n",
       "  ('rate', 1),\n",
       "  ('reach', 1),\n",
       "  ('read', 2),\n",
       "  ('recall', 1),\n",
       "  ('receiver_operate', 1),\n",
       "  ('reciprocate', 2),\n",
       "  ('red', 1),\n",
       "  ('relation', 2),\n",
       "  ('represent', 1),\n",
       "  ('result', 1),\n",
       "  ('roc_curve', 2),\n",
       "  ('sensitivity', 9),\n",
       "  ('separability', 4),\n",
       "  ('separation', 1),\n",
       "  ('setting', 1),\n",
       "  ('share', 1),\n",
       "  ('similarly', 1),\n",
       "  ('situation', 2),\n",
       "  ('specificity', 10),\n",
       "  ('speculate', 2),\n",
       "  ('statement', 1),\n",
       "  ('stumble', 1),\n",
       "  ('suggest', 1),\n",
       "  ('suggestion', 1),\n",
       "  ('tad', 1),\n",
       "  ('task', 1),\n",
       "  ('tell', 1),\n",
       "  ('term', 2),\n",
       "  ('thank', 1),\n",
       "  ('third', 1),\n",
       "  ('threshold', 6),\n",
       "  ('thus', 2),\n",
       "  ('tpr', 5),\n",
       "  ('true', 1),\n",
       "  ('twitter', 1),\n",
       "  ('twitter_linkedin', 1),\n",
       "  ('type', 2),\n",
       "  ('understand', 1),\n",
       "  ('understanding', 1),\n",
       "  ('use', 3),\n",
       "  ('value', 2),\n",
       "  ('various', 1),\n",
       "  ('vice_versa', 2),\n",
       "  ('well', 3),\n",
       "  ('whatsoever', 1),\n",
       "  ('write', 1)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(id2word_bi[id], freq) for id, freq in cp] for cp in corpus_bi[:1]]\n",
    "[[(id2word_tri[id], freq) for id, freq in cp] for cp in corpus_tri[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe6e59",
   "metadata": {},
   "source": [
    "### LSI & LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63c97011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_competition(corpus,id2word,data_lemmatized):\n",
    "    random_state = 100\n",
    "    for_framing = []\n",
    "    \n",
    "    algo_names = [\"LDA Model\",\"LSI Model\",]\n",
    "    algorithms = [genmodels.ldamodel.LdaModel,genmodels.lsimodel.\n",
    "                  LsiModel]\n",
    "    \n",
    "    for algo_name, algorithm in zip(algo_names, algorithms):\n",
    "        for k in range(1, 11):\n",
    "            if algo_name == 'LDA Model':\n",
    "                model = algorithm(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        \n",
    "                perplexity = model.log_perplexity(corpus)\n",
    "                coherence_model = CoherenceModel(model=model, texts=data_lemmatized, \n",
    "                               dictionary=id2word, coherence='c_v')\n",
    "                coherence = coherence_model.get_coherence()\n",
    "            if algo_name == 'LSI Model':\n",
    "                model = algorithm(corpus=corpus,\n",
    "                                  id2word=id2word,\n",
    "                                  num_topics=k,\n",
    "                                  chunksize=100)\n",
    "        \n",
    "                coherence_model = CoherenceModel(model=model, texts=data_lemmatized, \n",
    "                               dictionary=id2word, coherence='c_v')\n",
    "                coherence = coherence_model.get_coherence()\n",
    "        \n",
    "            data_row = {\n",
    "                'model': algo_name,\n",
    "                'topic count' : k,\n",
    "                'coherence' : coherence\n",
    "            }\n",
    "            for_framing.append(data_row)\n",
    "        \n",
    "    return pd.DataFrame(for_framing)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a3d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dataframe = model_competition(corpus_bi,id2word_bi,data_lemmatized_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2d42d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>topic count</th>\n",
       "      <th>coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LDA Model</td>\n",
       "      <td>9</td>\n",
       "      <td>0.604526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  topic count  coherence\n",
       "8  LDA Model            9   0.604526"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_dataframe.sort_values(by=['coherence'],ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4231c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_dataframe = model_competition(corpus_tri,id2word_tri,data_lemmatized_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "63250d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>topic count</th>\n",
       "      <th>coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LDA Model</td>\n",
       "      <td>9</td>\n",
       "      <td>0.604363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model  topic count  coherence\n",
       "8  LDA Model            9   0.604363"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_dataframe.sort_values(by=['coherence'],ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f722b",
   "metadata": {},
   "source": [
    "# HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f06346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.543526652939898\n"
     ]
    }
   ],
   "source": [
    "hdpmodel =  gensim.models.hdpmodel.HdpModel(corpus=corpus_bi, id2word=id2word_bi)\n",
    "coherence_model_hdpmodel = CoherenceModel(model=hdpmodel, texts=data_lemmatized_bi, dictionary=id2word_bi, coherence='c_v')\n",
    "coherence_hdpmodel = coherence_model_hdpmodel.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_hdpmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c89ae234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*datum + 0.010*model + 0.009*use + 0.005*data + 0.005*time + 0.005*learn + 0.005*make + 0.005*also + 0.004*image + 0.004*dataset + 0.004*need + 0.004*create + 0.004*value + 0.004*learning + 0.004*get + 0.004*function + 0.004*feature + 0.004*code + 0.004*work + 0.003*example'),\n",
       " (1,\n",
       "  '0.019*datum + 0.009*model + 0.009*data + 0.007*use + 0.007*science + 0.006*learn + 0.006*work + 0.005*time + 0.005*value + 0.004*get + 0.004*make + 0.004*learning + 0.004*project + 0.004*machine + 0.004*good + 0.004*well + 0.004*need + 0.003*find + 0.003*also + 0.003*feature'),\n",
       " (2,\n",
       "  '0.014*datum + 0.010*model + 0.009*use + 0.006*learn + 0.006*learning + 0.006*data + 0.005*function + 0.005*value + 0.005*time + 0.005*image + 0.004*create + 0.004*feature + 0.004*also + 0.004*dataset + 0.004*let + 0.004*make + 0.004*work + 0.004*see + 0.003*need + 0.003*machine'),\n",
       " (3,\n",
       "  '0.009*datum + 0.007*use + 0.006*time + 0.006*code + 0.005*value + 0.004*function + 0.004*create + 0.004*need + 0.004*get + 0.004*work + 0.004*make + 0.004*example + 0.003*learning + 0.003*let + 0.003*learn + 0.003*python + 0.003*data + 0.003*word + 0.003*model + 0.003*article'),\n",
       " (4,\n",
       "  '0.011*datum + 0.008*column + 0.005*python + 0.005*use + 0.005*select + 0.004*data + 0.004*also + 0.004*learn + 0.004*code + 0.004*make + 0.004*science + 0.004*time + 0.004*row + 0.004*panda + 0.004*create + 0.004*get + 0.003*dataframe + 0.003*see + 0.003*let + 0.003*good'),\n",
       " (5,\n",
       "  '0.008*use + 0.007*datum + 0.006*time + 0.006*python + 0.005*plot + 0.004*function + 0.004*feature + 0.004*work + 0.004*code + 0.004*model + 0.004*package + 0.004*virtual_environment + 0.004*value + 0.004*environment + 0.003*column + 0.003*make + 0.003*see + 0.003*let + 0.003*create + 0.003*project'),\n",
       " (6,\n",
       "  '0.014*model + 0.007*user + 0.005*base + 0.005*class + 0.005*item + 0.004*learning + 0.004*use + 0.004*method + 0.003*learn + 0.003*well + 0.003*score + 0.003*value + 0.003*machine + 0.003*new + 0.003*datum + 0.003*positive + 0.003*example + 0.002*make + 0.002*dataset + 0.002*weak_learner'),\n",
       " (7,\n",
       "  '0.009*datum + 0.006*model + 0.005*python + 0.004*use + 0.004*data + 0.003*learn + 0.003*package + 0.003*need + 0.003*time + 0.003*make + 0.003*run + 0.003*machine + 0.003*user + 0.003*image + 0.003*file + 0.003*install + 0.002*science + 0.002*project + 0.002*also + 0.002*version'),\n",
       " (8,\n",
       "  '0.010*dash + 0.008*datum + 0.006*code + 0.006*table + 0.005*file + 0.005*create + 0.005*use + 0.005*data + 0.005*graph + 0.004*python + 0.004*app + 0.003*dashboard + 0.003*image + 0.003*project + 0.003*plot + 0.003*function + 0.003*select + 0.002*user + 0.002*build + 0.002*plotly'),\n",
       " (9,\n",
       "  '0.007*feature + 0.005*cluster + 0.005*dataset + 0.005*use + 0.005*datum + 0.005*model + 0.004*value + 0.004*different + 0.003*variable + 0.003*join + 0.003*point + 0.002*age + 0.002*example + 0.002*tree + 0.002*number + 0.002*column + 0.002*method + 0.002*table + 0.002*result + 0.002*right'),\n",
       " (10,\n",
       "  '0.005*document + 0.005*word + 0.005*datum + 0.004*use + 0.003*give + 0.003*value + 0.003*distribution + 0.003*distance + 0.003*model + 0.003*title + 0.002*dataframe + 0.002*autoencoder + 0.002*measure + 0.002*set + 0.002*pca + 0.002*need + 0.002*image + 0.002*case + 0.002*body + 0.002*create'),\n",
       " (11,\n",
       "  '0.008*datum + 0.004*outlier + 0.003*need + 0.003*feature + 0.003*get + 0.003*data + 0.003*science + 0.003*score + 0.003*use + 0.003*course + 0.002*word + 0.002*look + 0.002*start + 0.002*see + 0.002*code + 0.002*dataset + 0.002*good + 0.002*want + 0.002*time + 0.002*let'),\n",
       " (12,\n",
       "  '0.006*time + 0.005*datum + 0.004*model + 0.004*use + 0.004*project + 0.003*python + 0.003*work + 0.002*dataset + 0.002*automate + 0.002*star + 0.002*task + 0.002*panda + 0.002*word + 0.002*help + 0.002*year + 0.002*transformer + 0.002*feature + 0.002*imputation + 0.002*need + 0.002*migration'),\n",
       " (13,\n",
       "  '0.005*value + 0.004*feature + 0.004*trade + 0.003*column + 0.003*datum + 0.003*time + 0.003*trading + 0.002*make + 0.002*correlation + 0.002*get + 0.002*function + 0.002*good + 0.002*market + 0.002*learning + 0.002*scale + 0.002*process + 0.002*mean + 0.002*price + 0.002*let + 0.002*day'),\n",
       " (14,\n",
       "  '0.008*datum + 0.005*model + 0.005*value + 0.004*use + 0.004*time + 0.003*string + 0.002*set + 0.002*also + 0.002*example + 0.002*dataset + 0.002*format + 0.002*layer + 0.002*equation + 0.002*different + 0.002*go + 0.002*data + 0.002*many + 0.001*case + 0.001*prediction + 0.001*number'),\n",
       " (15,\n",
       "  '0.008*function + 0.005*loss + 0.005*value + 0.005*model + 0.004*sample + 0.004*use + 0.003*datum + 0.003*prediction + 0.002*element + 0.002*example + 0.002*tree + 0.002*base + 0.002*error + 0.002*number + 0.002*case + 0.001*select + 0.001*give + 0.001*regression + 0.001*population + 0.001*probability'),\n",
       " (16,\n",
       "  '0.006*variable + 0.004*correlation + 0.003*inception + 0.003*use + 0.002*continuous + 0.002*image + 0.002*class + 0.002*model + 0.002*convolution + 0.002*metric + 0.002*categorical + 0.002*network + 0.002*loss + 0.002*check + 0.002*distance + 0.002*input + 0.002*datum + 0.002*test + 0.002*different + 0.002*layer'),\n",
       " (17,\n",
       "  '0.004*get + 0.004*project + 0.003*work + 0.003*learn + 0.003*datum + 0.003*portfolio + 0.003*science + 0.002*resume + 0.002*job + 0.002*way + 0.002*people + 0.002*data + 0.002*lot + 0.002*want + 0.002*skill + 0.002*experience + 0.002*apply + 0.002*find + 0.002*good + 0.002*column'),\n",
       " (18,\n",
       "  '0.005*datum + 0.004*model + 0.003*use + 0.002*make + 0.002*human + 0.002*test + 0.002*example + 0.002*set + 0.002*see + 0.001*machine + 0.001*train + 0.001*cross_validation + 0.001*training + 0.001*split + 0.001*number + 0.001*ai + 0.001*let + 0.001*fold + 0.001*go + 0.001*complaint'),\n",
       " (19,\n",
       "  '0.004*datum + 0.004*learn + 0.003*data + 0.003*exam + 0.003*science + 0.002*tensorflow + 0.002*machine + 0.002*course + 0.002*python + 0.002*scientist + 0.002*learning + 0.001*make + 0.001*problem + 0.001*time + 0.001*take + 0.001*concept + 0.001*use + 0.001*day + 0.001*know + 0.001*well')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdpmodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0332bf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5348782847089283\n"
     ]
    }
   ],
   "source": [
    "hdpmodel_tri =  gensim.models.hdpmodel.HdpModel(corpus=corpus_tri, id2word=id2word_tri)\n",
    "coherence_model_hdpmodel_tri = CoherenceModel(model=hdpmodel_tri, texts=data_lemmatized_tri, dictionary=id2word_tri, coherence='c_v')\n",
    "coherence_hdpmodel_tri = coherence_model_hdpmodel_tri.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_hdpmodel_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22e8ea1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.017*datum + 0.010*model + 0.009*use + 0.007*data + 0.006*time + 0.006*learn + 0.005*value + 0.005*learning + 0.004*make + 0.004*work + 0.004*also + 0.004*code + 0.004*image + 0.004*function + 0.004*need + 0.004*get + 0.004*science + 0.004*example + 0.004*well + 0.004*feature'),\n",
       " (1,\n",
       "  '0.008*model + 0.008*use + 0.007*datum + 0.006*python + 0.005*dataset + 0.005*user + 0.005*function + 0.005*create + 0.004*new + 0.004*project + 0.004*method + 0.004*want + 0.004*get + 0.004*see + 0.004*time + 0.004*need + 0.004*make + 0.003*also + 0.003*learn + 0.003*work'),\n",
       " (2,\n",
       "  '0.011*model + 0.010*datum + 0.007*use + 0.005*value + 0.005*image + 0.004*word + 0.004*make + 0.004*work + 0.004*data + 0.004*training + 0.003*time + 0.003*learning + 0.003*learn + 0.003*well + 0.003*also + 0.003*feature + 0.003*number + 0.003*set + 0.003*good + 0.003*dataset'),\n",
       " (3,\n",
       "  '0.015*datum + 0.008*data + 0.007*learn + 0.006*science + 0.005*work + 0.004*job + 0.004*get + 0.004*need + 0.004*use + 0.004*help + 0.003*good + 0.003*project + 0.003*time + 0.003*skill + 0.003*make + 0.003*many + 0.003*new + 0.003*scientist + 0.003*company + 0.003*create'),\n",
       " (4,\n",
       "  '0.009*value + 0.008*use + 0.006*column + 0.006*function + 0.006*datum + 0.006*row + 0.005*time + 0.004*word + 0.004*dataframe + 0.004*need + 0.004*get + 0.004*let + 0.003*example + 0.003*python + 0.003*document + 0.003*panda + 0.003*see + 0.003*mean + 0.003*take + 0.003*go'),\n",
       " (5,\n",
       "  '0.012*datum + 0.005*model + 0.005*machine + 0.004*learning + 0.004*use + 0.004*data + 0.003*learn + 0.003*also + 0.003*plot + 0.003*time + 0.003*prediction + 0.003*make + 0.003*value + 0.003*loss + 0.003*work + 0.003*attribute + 0.003*visualize + 0.002*function + 0.002*dimension + 0.002*see'),\n",
       " (6,\n",
       "  '0.008*datum + 0.008*use + 0.005*create + 0.004*plot + 0.004*function + 0.004*dataframe + 0.004*table + 0.004*need + 0.003*panda + 0.003*set + 0.003*python + 0.003*code + 0.003*also + 0.003*data + 0.003*step + 0.003*make + 0.003*file + 0.002*model + 0.002*spark + 0.002*chart'),\n",
       " (7,\n",
       "  '0.010*datum + 0.006*value + 0.006*use + 0.005*column + 0.004*variable + 0.004*time + 0.003*panda + 0.003*code + 0.003*image + 0.003*outlier + 0.003*example + 0.003*run + 0.003*see + 0.003*model + 0.003*get + 0.003*author + 0.003*create + 0.003*set + 0.003*method + 0.003*look'),\n",
       " (8,\n",
       "  '0.009*model + 0.008*datum + 0.004*distribution + 0.004*generate + 0.004*training + 0.003*learn + 0.003*train + 0.003*see + 0.003*random + 0.003*variable + 0.003*problem + 0.003*true + 0.003*data + 0.003*code + 0.003*science + 0.002*make + 0.002*let + 0.002*take + 0.002*set + 0.002*use'),\n",
       " (9,\n",
       "  '0.010*model + 0.007*datum + 0.006*class + 0.006*feature + 0.006*use + 0.004*decision_tree + 0.003*time + 0.003*prediction + 0.003*tree + 0.003*make + 0.003*image + 0.003*value + 0.003*example + 0.003*dataset + 0.003*panda + 0.002*method + 0.002*machine + 0.002*object + 0.002*training + 0.002*number'),\n",
       " (10,\n",
       "  '0.008*datum + 0.005*cluster + 0.004*feature + 0.004*model + 0.004*time + 0.004*value + 0.004*use + 0.003*see + 0.003*variable + 0.003*set + 0.003*different + 0.003*point + 0.002*first + 0.002*activity + 0.002*data + 0.002*let + 0.002*example + 0.002*look + 0.002*column + 0.002*dataset'),\n",
       " (11,\n",
       "  '0.006*datum + 0.005*work + 0.004*project + 0.004*get + 0.003*science + 0.003*data + 0.003*people + 0.003*make + 0.003*job + 0.003*need + 0.002*lot + 0.002*skill + 0.002*well + 0.002*portfolio + 0.002*help + 0.002*good + 0.002*resume + 0.002*want + 0.002*learn + 0.002*go'),\n",
       " (12,\n",
       "  '0.007*datum + 0.007*column + 0.005*value + 0.004*code + 0.004*function + 0.004*dataframe + 0.004*create + 0.003*example + 0.003*want + 0.003*panda + 0.003*use + 0.002*make + 0.002*work + 0.002*author + 0.002*number + 0.002*list + 0.002*need + 0.002*data + 0.002*python + 0.002*time'),\n",
       " (13,\n",
       "  '0.008*variable + 0.004*datum + 0.003*correlation + 0.003*feature + 0.003*use + 0.002*pca + 0.002*continuous + 0.002*categorical + 0.002*want + 0.002*different + 0.002*metric + 0.002*layer + 0.002*method + 0.002*column + 0.002*principal_component + 0.002*work + 0.002*distance + 0.002*important + 0.002*many + 0.002*direction'),\n",
       " (14,\n",
       "  '0.009*datum + 0.007*table + 0.006*data + 0.006*dash + 0.004*graph + 0.004*science + 0.004*course + 0.004*code + 0.003*use + 0.003*dashboard + 0.003*app + 0.003*block + 0.002*date + 0.002*file + 0.002*first + 0.002*review + 0.002*function + 0.002*build + 0.002*change + 0.002*metric'),\n",
       " (15,\n",
       "  '0.006*model + 0.005*time + 0.003*use + 0.003*function + 0.003*neural_network + 0.002*deep + 0.002*learning + 0.002*value + 0.002*predict + 0.002*feature + 0.002*series + 0.002*build + 0.002*input + 0.002*prediction + 0.002*weight + 0.002*datum + 0.002*section + 0.002*loss + 0.001*know + 0.001*layer'),\n",
       " (16,\n",
       "  '0.005*datum + 0.004*panda + 0.004*machine + 0.003*learning + 0.003*learn + 0.003*python + 0.002*get + 0.002*point + 0.002*line + 0.002*column + 0.002*week + 0.002*use + 0.002*data + 0.002*value + 0.002*make + 0.002*project + 0.002*also + 0.002*function + 0.001*dataset + 0.001*science'),\n",
       " (17,\n",
       "  '0.004*trade + 0.003*trading + 0.003*time + 0.002*datum + 0.002*science + 0.002*market + 0.002*make + 0.002*course + 0.002*learn + 0.002*get + 0.002*price + 0.002*day + 0.002*take + 0.001*risk + 0.001*good + 0.001*lose + 0.001*way + 0.001*work + 0.001*money + 0.001*code'),\n",
       " (18,\n",
       "  '0.004*model + 0.004*datum + 0.004*value + 0.003*variable + 0.003*use + 0.003*linear_regression + 0.003*score + 0.003*feature + 0.003*coefficient + 0.003*alpha + 0.002*set + 0.002*imputation + 0.002*data + 0.002*dataset + 0.002*import + 0.002*print + 0.002*predict + 0.002*regression + 0.002*miss + 0.002*case'),\n",
       " (19,\n",
       "  '0.009*column + 0.008*select + 0.006*row + 0.004*dataframe + 0.004*label + 0.004*datum + 0.003*use + 0.003*selection + 0.003*series + 0.003*single + 0.003*panda + 0.003*list + 0.002*iloc + 0.002*index + 0.002*let + 0.002*value + 0.002*integer + 0.002*name + 0.002*also + 0.002*python')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdpmodel_tri.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a9ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
